WARNING: CPU IP/backtrace sampling not supported, disabling.
Try the 'nsys status --environment' command to learn more.

WARNING: CPU context switch tracing not supported, disabling.
Try the 'nsys status --environment' command to learn more.

ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
------------ Options -------------
abstractor: True
base_dim: 512
batch_size: 64
batch_size_eval: 32
beta_schedule: linear
cfg_scale: 2.5
cfg_scheduler_type: none
checkpoints_dir: ./checkpoints
clip_grad_norm: 1
cond_mask_prob: 0.1
continue_ckpt: latest.tar
dataset_name: t2m
decay_rate: 0.9
diffuser_name: dpmsolver
diffusion_steps: 1000
dim_mults: [2, 2, 2, 2]
disable_sta: False
diversity_times: 300
dropout: 0.1
enable_cfg_scheduler: False
enable_trace: False
eval_meta_dir: ./data
evaluator_dir: ./data/pretrained_models
evl_mode: fid
feat_bias: 5
glove_dir: ./data/glove
is_continue: False
laten_size: 77
latent_dim: 512
log_every: 500
lr: 0.0001
mm_num_repeats: 30
mm_num_samples: 100
mm_num_times: 10
model_ema: False
model_ema_decay: 0.9999
model_ema_steps: 32
name: t2m
no_adagn: False
no_eff: False
no_ema: False
no_fp16: False
num_inference_steps: 10
num_layers: 8
num_train_steps: 50
opt_path: 
prediction_type: sample
replication_times: 20
save_interval: 5000
seed: 0
text_encoder_type: t5
text_latent_dim: 256
time_dim: 512
update_lr_steps: 5000
use_text_cache: True
weight_decay: 0.01
which_ckpt: latest_120000
-------------- End ----------------

 Loading train mode HumanML3D train dataset ...
  0%|          | 0/23384 [00:00<?, ?it/s]  1%|          | 201/23384 [00:00<00:11, 2002.41it/s]  2%|▏         | 402/23384 [00:00<00:11, 1968.68it/s]  3%|▎         | 608/23384 [00:00<00:11, 2008.52it/s]  4%|▎         | 822/23384 [00:00<00:10, 2059.80it/s]  4%|▍         | 1031/23384 [00:00<00:10, 2070.13it/s]  5%|▌         | 1248/23384 [00:00<00:10, 2103.46it/s]  6%|▌         | 1459/23384 [00:00<00:10, 2103.84it/s]  7%|▋         | 1670/23384 [00:00<00:10, 2101.59it/s]  8%|▊         | 1884/23384 [00:00<00:10, 2113.03it/s]  9%|▉         | 2101/23384 [00:01<00:09, 2130.16it/s] 10%|▉         | 2321/23384 [00:01<00:09, 2150.04it/s] 11%|█         | 2538/23384 [00:01<00:09, 2154.38it/s] 12%|█▏        | 2758/23384 [00:01<00:09, 2167.04it/s] 13%|█▎        | 2975/23384 [00:01<00:09, 2142.48it/s] 14%|█▎        | 3191/23384 [00:01<00:09, 2147.44it/s] 15%|█▍        | 3411/23384 [00:01<00:09, 2160.99it/s] 16%|█▌        | 3628/23384 [00:01<00:09, 2145.50it/s] 16%|█▋        | 3849/23384 [00:01<00:09, 2163.02it/s] 17%|█▋        | 4066/23384 [00:01<00:08, 2150.79it/s] 18%|█▊        | 4282/23384 [00:02<00:09, 2119.42it/s] 19%|█▉        | 4495/23384 [00:02<00:09, 2045.49it/s] 20%|██        | 4701/23384 [00:02<00:09, 2029.45it/s] 21%|██        | 4919/23384 [00:02<00:08, 2072.29it/s] 22%|██▏       | 5131/23384 [00:02<00:08, 2085.68it/s] 23%|██▎       | 5340/23384 [00:02<00:08, 2067.85it/s] 24%|██▍       | 5558/23384 [00:02<00:08, 2097.47it/s] 25%|██▍       | 5768/23384 [00:02<00:08, 2092.47it/s] 26%|██▌       | 5980/23384 [00:02<00:08, 2100.59it/s] 26%|██▋       | 6192/23384 [00:02<00:08, 2103.45it/s] 27%|██▋       | 6409/23384 [00:03<00:07, 2122.07it/s] 28%|██▊       | 6622/23384 [00:03<00:07, 2122.76it/s] 29%|██▉       | 6842/23384 [00:03<00:07, 2145.79it/s] 30%|███       | 7064/23384 [00:03<00:07, 2166.24it/s] 31%|███       | 7284/23384 [00:03<00:07, 2175.06it/s] 32%|███▏      | 7502/23384 [00:03<00:07, 2149.51it/s] 33%|███▎      | 7718/23384 [00:03<00:07, 2146.88it/s] 34%|███▍      | 7933/23384 [00:04<00:14, 1079.03it/s] 35%|███▍      | 8145/23384 [00:04<00:12, 1261.37it/s] 36%|███▌      | 8355/23384 [00:04<00:10, 1428.55it/s] 37%|███▋      | 8578/23384 [00:04<00:09, 1606.87it/s] 38%|███▊      | 8798/23384 [00:04<00:08, 1749.39it/s] 39%|███▊      | 9014/23384 [00:04<00:07, 1852.05it/s] 39%|███▉      | 9222/23384 [00:04<00:07, 1907.58it/s] 40%|████      | 9430/23384 [00:04<00:07, 1943.22it/s] 41%|████      | 9640/23384 [00:04<00:06, 1986.99it/s] 42%|████▏     | 9848/23384 [00:04<00:06, 2012.40it/s] 43%|████▎     | 10062/23384 [00:05<00:06, 2049.43it/s] 44%|████▍     | 10272/23384 [00:05<00:06, 2059.76it/s] 45%|████▍     | 10483/23384 [00:05<00:06, 2073.04it/s] 46%|████▌     | 10693/23384 [00:05<00:06, 2074.93it/s] 47%|████▋     | 10903/23384 [00:05<00:06, 2072.27it/s] 48%|████▊     | 11115/23384 [00:05<00:05, 2084.86it/s] 48%|████▊     | 11326/23384 [00:05<00:05, 2090.88it/s] 49%|████▉     | 11536/23384 [00:05<00:05, 2070.35it/s] 50%|█████     | 11750/23384 [00:05<00:05, 2090.37it/s] 51%|█████     | 11962/23384 [00:05<00:05, 2096.64it/s] 52%|█████▏    | 12176/23384 [00:06<00:05, 2108.83it/s] 53%|█████▎    | 12388/23384 [00:06<00:05, 2080.19it/s] 54%|█████▍    | 12604/23384 [00:06<00:05, 2103.29it/s] 55%|█████▍    | 12820/23384 [00:06<00:04, 2117.33it/s] 56%|█████▌    | 13042/23384 [00:06<00:04, 2146.95it/s] 57%|█████▋    | 13257/23384 [00:06<00:04, 2100.17it/s] 58%|█████▊    | 13491/23384 [00:06<00:04, 2169.66it/s] 59%|█████▉    | 13745/23384 [00:06<00:04, 2276.33it/s] 60%|█████▉    | 14026/23384 [00:06<00:03, 2434.33it/s] 61%|██████    | 14310/23384 [00:07<00:03, 2554.31it/s] 62%|██████▏   | 14588/23384 [00:07<00:03, 2621.56it/s] 64%|██████▎   | 14868/23384 [00:07<00:03, 2673.37it/s] 65%|██████▍   | 15145/23384 [00:07<00:03, 2701.96it/s] 66%|██████▌   | 15427/23384 [00:07<00:02, 2734.86it/s] 67%|██████▋   | 15705/23384 [00:07<00:02, 2745.24it/s] 68%|██████▊   | 15989/23384 [00:07<00:02, 2772.55it/s] 70%|██████▉   | 16273/23384 [00:07<00:02, 2790.68it/s] 71%|███████   | 16554/23384 [00:07<00:02, 2795.16it/s] 72%|███████▏  | 16836/23384 [00:07<00:02, 2800.84it/s] 73%|███████▎  | 17117/23384 [00:08<00:02, 2796.73it/s] 74%|███████▍  | 17407/23384 [00:08<00:02, 2827.14it/s] 76%|███████▌  | 17690/23384 [00:08<00:02, 2816.16it/s] 77%|███████▋  | 17973/23384 [00:08<00:01, 2818.44it/s] 78%|███████▊  | 18255/23384 [00:08<00:01, 2798.69it/s] 79%|███████▉  | 18541/23384 [00:08<00:01, 2815.65it/s] 81%|████████  | 18829/23384 [00:08<00:01, 2832.10it/s] 82%|████████▏ | 19113/23384 [00:08<00:01, 2824.29it/s] 83%|████████▎ | 19399/23384 [00:08<00:01, 2833.01it/s] 84%|████████▍ | 19687/23384 [00:08<00:01, 2845.45it/s] 85%|████████▌ | 19972/23384 [00:09<00:01, 2833.61it/s] 87%|████████▋ | 20263/23384 [00:09<00:01, 2852.89it/s] 88%|████████▊ | 20558/23384 [00:09<00:00, 2881.20it/s] 89%|████████▉ | 20847/23384 [00:09<00:00, 2845.30it/s] 90%|█████████ | 21132/23384 [00:09<00:00, 2815.43it/s] 92%|█████████▏| 21414/23384 [00:09<00:00, 2815.21it/s] 93%|█████████▎| 21700/23384 [00:09<00:00, 2827.28it/s] 94%|█████████▍| 21983/23384 [00:09<00:00, 2802.65it/s] 95%|█████████▌| 22264/23384 [00:09<00:00, 2791.98it/s] 96%|█████████▋| 22548/23384 [00:09<00:00, 2803.82it/s] 98%|█████████▊| 22829/23384 [00:10<00:00, 2803.66it/s] 99%|█████████▉| 23113/23384 [00:10<00:00, 2812.19it/s]100%|██████████| 23384/23384 [00:10<00:00, 2285.37it/s]
Completing loading t2m dataset
Calculating dataset hash:   0%|          | 0/24546 [00:00<?, ?it/s]Calculating dataset hash:   5%|▌         | 1281/24546 [00:00<00:01, 12808.88it/s]Calculating dataset hash:  10%|█         | 2562/24546 [00:00<00:01, 11841.69it/s]Calculating dataset hash:  15%|█▌        | 3752/24546 [00:00<00:01, 10915.74it/s]Calculating dataset hash:  20%|█▉        | 4852/24546 [00:00<00:01, 10388.37it/s]Calculating dataset hash:  24%|██▍       | 5896/24546 [00:00<00:01, 9905.69it/s] Calculating dataset hash:  28%|██▊       | 6891/24546 [00:00<00:01, 9501.97it/s]Calculating dataset hash:  32%|███▏      | 7844/24546 [00:00<00:01, 9229.72it/s]Calculating dataset hash:  36%|███▌      | 8768/24546 [00:00<00:01, 8867.14it/s]Calculating dataset hash:  39%|███▉      | 9656/24546 [00:01<00:01, 8520.59it/s]Calculating dataset hash:  43%|████▎     | 10509/24546 [00:01<00:01, 8264.67it/s]Calculating dataset hash:  46%|████▌     | 11336/24546 [00:01<00:01, 7893.97it/s]Calculating dataset hash:  49%|████▉     | 12127/24546 [00:01<00:01, 7597.14it/s]Calculating dataset hash:  53%|█████▎    | 12888/24546 [00:01<00:01, 7187.58it/s]Calculating dataset hash:  55%|█████▌    | 13609/24546 [00:01<00:01, 6878.89it/s]Calculating dataset hash:  58%|█████▊    | 14299/24546 [00:01<00:01, 6595.43it/s]Calculating dataset hash:  61%|██████    | 14960/24546 [00:01<00:01, 6285.53it/s]Calculating dataset hash:  64%|██████▎   | 15590/24546 [00:01<00:01, 6077.18it/s]Calculating dataset hash:  66%|██████▌   | 16199/24546 [00:02<00:01, 5943.96it/s]Calculating dataset hash:  68%|██████▊   | 16793/24546 [00:02<00:01, 5313.89it/s]Calculating dataset hash:  71%|███████   | 17334/24546 [00:02<00:01, 4977.22it/s]Calculating dataset hash:  73%|███████▎  | 17839/24546 [00:02<00:01, 4750.97it/s]Calculating dataset hash:  75%|███████▍  | 18319/24546 [00:02<00:01, 4572.17it/s]Calculating dataset hash:  77%|███████▋  | 18779/24546 [00:02<00:01, 4479.13it/s]Calculating dataset hash:  78%|███████▊  | 19228/24546 [00:02<00:01, 4407.72it/s]Calculating dataset hash:  80%|████████  | 19682/24546 [00:02<00:01, 4443.30it/s]Calculating dataset hash:  83%|████████▎ | 20394/24546 [00:02<00:00, 5194.57it/s]Calculating dataset hash:  86%|████████▌ | 21106/24546 [00:03<00:00, 5745.30it/s]Calculating dataset hash:  89%|████████▉ | 21819/24546 [00:03<00:00, 6146.73it/s]Calculating dataset hash:  92%|█████████▏| 22532/24546 [00:03<00:00, 6433.86it/s]Calculating dataset hash:  95%|█████████▍| 23247/24546 [00:03<00:00, 6642.85it/s]Calculating dataset hash:  98%|█████████▊| 23949/24546 [00:03<00:00, 6754.54it/s]Calculating dataset hash: 100%|██████████| 24546/24546 [00:03<00:00, 6894.30it/s]
[Cache]  读取已有缓存: ./text_cache/t2m/train/t5/raw_embeds.pt

Initializing model ...
Namespace(name='t2m', dataset_name='t2m', feat_bias=5, checkpoints_dir='./checkpoints', log_every=500, save_interval=5000, num_layers=8, latent_dim=512, text_latent_dim=256, time_dim=512, base_dim=512, dim_mults=[2, 2, 2, 2], no_eff=False, no_adagn=False, diffusion_steps=1000, prediction_type='sample', text_encoder_type='t5', abstractor=True, disable_sta=False, laten_size=77, dropout=0.1, seed=0, num_train_steps=50, lr=0.0001, decay_rate=0.9, update_lr_steps=5000, cond_mask_prob=0.1, clip_grad_norm=1, weight_decay=0.01, batch_size=64, beta_schedule='linear', enable_trace=False, use_text_cache=True, is_continue=False, continue_ckpt='latest.tar', opt_path='', which_ckpt='latest_120000', evaluator_dir='./data/pretrained_models', eval_meta_dir='./data', glove_dir='./data/glove', num_inference_steps=10, diffuser_name='dpmsolver', no_ema=False, no_fp16=False, enable_cfg_scheduler=False, cfg_scheduler_type='none', replication_times=20, batch_size_eval=32, diversity_times=300, mm_num_samples=100, mm_num_repeats=30, mm_num_times=10, evl_mode='fid', cfg_scale=2.5, model_ema=False, model_ema_steps=32, model_ema_decay=0.9999, unit_length=4, max_text_len=20, text_enc_mod='bigru', estimator_mod='bigru', dim_text_hidden=512, dim_att_vec=512, dim_z=128, dim_movement_enc_hidden=512, dim_movement_dec_hidden=512, dim_movement_latent=512, dim_word=300, dim_pos_ohot=15, dim_motion_hidden=1024, dim_coemb_hidden=512, joints_num=22, dim_pose=263, max_motion_length=196, radius=4, fps=20, device=device(type='cuda'), is_train=True, save_root='./checkpoints/t2m/t2m', model_dir='./checkpoints/t2m/t2m/model', meta_dir='./checkpoints/t2m/t2m/meta')
Creating UNet with text encoder: 't5'
Loading T5 model from: ./T5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 24.12it/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
wandb: Currently logged in as: ccs_covenant (ccs_covenant-hong-kong-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /data/kuimou/openANT/wandb/run-20250722_092344-hzirgvzt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t2m_20250722-09_65c793
wandb: ⭐️ View project at https://wandb.ai/ccs_covenant-hong-kong-university-of-science-and-technology/t2m
wandb: 🚀 View run at https://wandb.ai/ccs_covenant-hong-kong-university-of-science-and-technology/t2m/runs/hzirgvzt
Building STA connector.
[CondUnet1D] Dims: [263, 1024, 1024, 1024, 1024], Multipliers: [2, 2, 2, 2]
T2M-Unet model created successfully.
Finish building Model.

Diffusion_config:
 FrozenDict([('num_train_timesteps', 1000), ('beta_start', 0.0001), ('beta_end', 0.02), ('beta_schedule', 'linear'), ('trained_betas', None), ('variance_type', 'fixed_small'), ('clip_sample', False), ('prediction_type', 'sample'), ('thresholding', False), ('dynamic_thresholding_ratio', 0.995), ('clip_sample_range', 1.0), ('sample_max_value', 1.0), ('timestep_spacing', 'leading'), ('steps_offset', 0), ('rescale_betas_zero_snr', False), ('_use_default_values', ['sample_max_value', 'rescale_betas_zero_snr', 'timestep_spacing', 'beta_start', 'dynamic_thresholding_ratio', 'thresholding', 'steps_offset', 'beta_end', 'trained_betas', 'clip_sample_range'])])
Start experiment: 2025-07-22_09:23:45
Setting up evaluation components...

Loading Evaluation Model Wrapper (Epoch 28) Completed!!

 Loading gt_eval mode HumanML3D dataset ...
./data/t2m_std.npy
  0%|          | 0/1460 [00:00<?, ?it/s] 30%|███       | 444/1460 [00:00<00:00, 4432.34it/s] 62%|██████▏   | 901/1460 [00:00<00:00, 4510.03it/s] 93%|█████████▎| 1358/1460 [00:00<00:00, 4533.38it/s]100%|██████████| 1460/1460 [00:00<00:00, 4412.96it/s]
Completing loading t2m dataset
Calculating dataset hash:   0%|          | 0/1530 [00:00<?, ?it/s]Calculating dataset hash:  31%|███       | 470/1530 [00:00<00:00, 4696.63it/s]Calculating dataset hash:  61%|██████▏   | 940/1530 [00:00<00:00, 4332.11it/s]Calculating dataset hash:  90%|████████▉ | 1376/1530 [00:00<00:00, 4122.62it/s]Calculating dataset hash: 100%|██████████| 1530/1530 [00:00<00:00, 4172.16it/s]
[Cache]  读取已有缓存: ./text_cache/t2m/val/t5/raw_embeds.pt

 Loading eval mode HumanML3D dataset ...
./checkpoints/t2m/t2m/meta/std.npy
  0%|          | 0/1460 [00:00<?, ?it/s] 23%|██▎       | 342/1460 [00:00<00:00, 3412.33it/s] 47%|████▋     | 684/1460 [00:00<00:00, 3343.23it/s] 70%|██████▉   | 1020/1460 [00:00<00:00, 3345.95it/s] 93%|█████████▎| 1357/1460 [00:00<00:00, 3354.72it/s]100%|██████████| 1460/1460 [00:00<00:00, 3353.32it/s]
Completing loading t2m dataset
Evaluation components are ready.
Need to train for 1 epochs....
[2025-07-22 09:28:40,978] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-22 09:28:43,863] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mt2m_20250722-09_65c793[0m at: [34mhttps://wandb.ai/ccs_covenant-hong-kong-university-of-science-and-technology/t2m/runs/hzirgvzt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250722_092344-hzirgvzt/logs[0m
FINISH
Generated:
