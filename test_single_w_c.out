ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
------------ Options -------------
abstractor: True
base_dim: 512
batch_size: 64
batch_size_eval: 32
beta_schedule: linear
cfg_scale: 2.5
cfg_scheduler_type: none
checkpoints_dir: ./checkpoints
clip_grad_norm: 1
cond_mask_prob: 0.1
continue_ckpt: latest.tar
dataset_name: t2m
decay_rate: 0.9
diffuser_name: dpmsolver
diffusion_steps: 1000
dim_mults: [2, 2, 2, 2]
disable_sta: False
diversity_times: 300
dropout: 0.1
enable_cfg_scheduler: False
enable_trace: False
eval_meta_dir: ./data
evaluator_dir: ./data/pretrained_models
evl_mode: fid
feat_bias: 5
glove_dir: ./data/glove
is_continue: False
laten_size: 77
latent_dim: 512
log_every: 500
lr: 0.0001
mm_num_repeats: 30
mm_num_samples: 100
mm_num_times: 10
model_ema: False
model_ema_decay: 0.9999
model_ema_steps: 32
name: t2m
no_adagn: False
no_eff: False
no_ema: False
no_fp16: False
num_inference_steps: 10
num_layers: 8
num_train_steps: 200000
opt_path: 
prediction_type: sample
replication_times: 20
save_interval: 5000
seed: 0
text_encoder_type: t5
text_latent_dim: 256
time_dim: 512
update_lr_steps: 5000
use_text_cache: True
weight_decay: 0.01
which_ckpt: latest_120000
-------------- End ----------------

 Loading train mode HumanML3D train dataset ...
  0%|          | 0/23384 [00:00<?, ?it/s]  2%|▏         | 380/23384 [00:00<00:06, 3794.11it/s]  3%|▎         | 760/23384 [00:00<00:06, 3745.55it/s]  5%|▍         | 1157/23384 [00:00<00:05, 3841.33it/s]  7%|▋         | 1548/23384 [00:00<00:05, 3864.09it/s]  8%|▊         | 1936/23384 [00:00<00:05, 3869.45it/s] 10%|▉         | 2333/23384 [00:00<00:05, 3901.22it/s] 12%|█▏        | 2724/23384 [00:00<00:05, 3857.29it/s] 13%|█▎        | 3111/23384 [00:00<00:05, 3858.99it/s] 15%|█▍        | 3500/23384 [00:00<00:05, 3865.32it/s] 17%|█▋        | 3887/23384 [00:01<00:05, 3854.83it/s] 18%|█▊        | 4295/23384 [00:01<00:04, 3923.27it/s] 20%|██        | 4688/23384 [00:01<00:04, 3889.47it/s] 22%|██▏       | 5085/23384 [00:01<00:04, 3912.75it/s] 23%|██▎       | 5477/23384 [00:01<00:04, 3868.71it/s] 25%|██▌       | 5865/23384 [00:01<00:04, 3850.14it/s] 27%|██▋       | 6265/23384 [00:01<00:04, 3893.51it/s] 28%|██▊       | 6655/23384 [00:01<00:04, 3861.63it/s] 30%|███       | 7063/23384 [00:01<00:04, 3924.27it/s] 32%|███▏      | 7456/23384 [00:01<00:04, 3888.53it/s] 34%|███▎      | 7846/23384 [00:02<00:06, 2521.61it/s] 35%|███▌      | 8243/23384 [00:02<00:05, 2832.78it/s] 37%|███▋      | 8629/23384 [00:02<00:04, 3073.98it/s] 39%|███▊      | 9027/23384 [00:02<00:04, 3300.91it/s] 40%|████      | 9394/23384 [00:02<00:04, 3398.13it/s] 42%|████▏     | 9764/23384 [00:02<00:03, 3478.58it/s] 43%|████▎     | 10156/23384 [00:02<00:03, 3601.22it/s] 45%|████▌     | 10531/23384 [00:02<00:03, 3614.54it/s] 47%|████▋     | 10923/23384 [00:03<00:03, 3700.67it/s] 48%|████▊     | 11311/23384 [00:03<00:03, 3749.66it/s] 50%|█████     | 11692/23384 [00:03<00:03, 3739.84it/s] 52%|█████▏    | 12088/23384 [00:03<00:02, 3802.15it/s] 53%|█████▎    | 12471/23384 [00:03<00:02, 3805.17it/s] 55%|█████▌    | 12863/23384 [00:03<00:02, 3838.66it/s] 57%|█████▋    | 13249/23384 [00:03<00:02, 3802.77it/s] 58%|█████▊    | 13631/23384 [00:03<00:02, 3783.27it/s] 60%|█████▉    | 14011/23384 [00:03<00:02, 3763.44it/s] 62%|██████▏   | 14388/23384 [00:03<00:02, 3685.21it/s] 63%|██████▎   | 14767/23384 [00:04<00:02, 3713.57it/s] 65%|██████▍   | 15139/23384 [00:04<00:02, 3693.41it/s] 66%|██████▋   | 15513/23384 [00:04<00:02, 3706.66it/s] 68%|██████▊   | 15918/23384 [00:04<00:01, 3805.38it/s] 70%|██████▉   | 16304/23384 [00:04<00:01, 3819.46it/s] 71%|███████▏  | 16698/23384 [00:04<00:01, 3854.85it/s] 73%|███████▎  | 17084/23384 [00:04<00:01, 3852.87it/s] 75%|███████▍  | 17483/23384 [00:04<00:01, 3892.63it/s] 76%|███████▋  | 17873/23384 [00:04<00:01, 3855.72it/s] 78%|███████▊  | 18259/23384 [00:04<00:01, 3792.00it/s] 80%|███████▉  | 18671/23384 [00:05<00:01, 3886.50it/s] 82%|████████▏ | 19068/23384 [00:05<00:01, 3907.65it/s] 83%|████████▎ | 19460/23384 [00:05<00:01, 3898.15it/s] 85%|████████▍ | 19865/23384 [00:05<00:00, 3940.68it/s] 87%|████████▋ | 20260/23384 [00:05<00:00, 3909.73it/s] 88%|████████▊ | 20671/23384 [00:05<00:00, 3965.68it/s] 90%|█████████ | 21068/23384 [00:05<00:00, 3912.72it/s] 92%|█████████▏| 21460/23384 [00:05<00:00, 3861.80it/s] 93%|█████████▎| 21853/23384 [00:05<00:00, 3880.95it/s] 95%|█████████▌| 22242/23384 [00:05<00:00, 3836.15it/s] 97%|█████████▋| 22638/23384 [00:06<00:00, 3870.00it/s] 98%|█████████▊| 23026/23384 [00:06<00:00, 3806.84it/s]100%|██████████| 23384/23384 [00:06<00:00, 3736.13it/s]
Completing loading t2m dataset
Calculating dataset hash:   0%|          | 0/24546 [00:00<?, ?it/s]Calculating dataset hash:   8%|▊         | 1852/24546 [00:00<00:01, 18513.44it/s]Calculating dataset hash:  15%|█▌        | 3704/24546 [00:00<00:01, 16328.94it/s]Calculating dataset hash:  22%|██▏       | 5354/24546 [00:00<00:01, 14979.98it/s]Calculating dataset hash:  28%|██▊       | 6867/24546 [00:00<00:01, 14021.23it/s]Calculating dataset hash:  34%|███▎      | 8280/24546 [00:00<00:01, 13549.21it/s]Calculating dataset hash:  39%|███▉      | 9640/24546 [00:00<00:01, 12965.81it/s]Calculating dataset hash:  45%|████▍     | 10940/24546 [00:00<00:01, 12412.63it/s]Calculating dataset hash:  50%|████▉     | 12183/24546 [00:00<00:01, 11696.90it/s]Calculating dataset hash:  54%|█████▍    | 13357/24546 [00:01<00:01, 10952.32it/s]Calculating dataset hash:  59%|█████▉    | 14458/24546 [00:01<00:00, 10276.76it/s]Calculating dataset hash:  63%|██████▎   | 15492/24546 [00:01<00:00, 9809.75it/s] Calculating dataset hash:  67%|██████▋   | 16477/24546 [00:01<00:00, 9015.26it/s]Calculating dataset hash:  71%|███████   | 17387/24546 [00:01<00:00, 8441.91it/s]Calculating dataset hash:  75%|███████▍  | 18380/24546 [00:01<00:00, 8822.38it/s]Calculating dataset hash:  79%|███████▉  | 19354/24546 [00:01<00:00, 9067.26it/s]Calculating dataset hash:  83%|████████▎ | 20323/24546 [00:01<00:00, 9237.42it/s]Calculating dataset hash:  87%|████████▋ | 21292/24546 [00:01<00:00, 9364.62it/s]Calculating dataset hash:  91%|█████████ | 22285/24546 [00:02<00:00, 9527.45it/s]Calculating dataset hash:  95%|█████████▍| 23247/24546 [00:02<00:00, 9553.37it/s]Calculating dataset hash:  99%|█████████▉| 24243/24546 [00:02<00:00, 9672.66it/s]Calculating dataset hash: 100%|██████████| 24546/24546 [00:02<00:00, 10731.70it/s]
[Cache]  读取已有缓存: ./text_cache/t2m/train/t5/raw_embeds.pt

Initializing model ...
Namespace(name='t2m', dataset_name='t2m', feat_bias=5, checkpoints_dir='./checkpoints', log_every=500, save_interval=5000, num_layers=8, latent_dim=512, text_latent_dim=256, time_dim=512, base_dim=512, dim_mults=[2, 2, 2, 2], no_eff=False, no_adagn=False, diffusion_steps=1000, prediction_type='sample', text_encoder_type='t5', abstractor=True, disable_sta=False, laten_size=77, dropout=0.1, seed=0, num_train_steps=200000, lr=0.0001, decay_rate=0.9, update_lr_steps=5000, cond_mask_prob=0.1, clip_grad_norm=1, weight_decay=0.01, batch_size=64, beta_schedule='linear', enable_trace=False, use_text_cache=True, is_continue=False, continue_ckpt='latest.tar', opt_path='', which_ckpt='latest_120000', evaluator_dir='./data/pretrained_models', eval_meta_dir='./data', glove_dir='./data/glove', num_inference_steps=10, diffuser_name='dpmsolver', no_ema=False, no_fp16=False, enable_cfg_scheduler=False, cfg_scheduler_type='none', replication_times=20, batch_size_eval=32, diversity_times=300, mm_num_samples=100, mm_num_repeats=30, mm_num_times=10, evl_mode='fid', cfg_scale=2.5, model_ema=False, model_ema_steps=32, model_ema_decay=0.9999, unit_length=4, max_text_len=20, text_enc_mod='bigru', estimator_mod='bigru', dim_text_hidden=512, dim_att_vec=512, dim_z=128, dim_movement_enc_hidden=512, dim_movement_dec_hidden=512, dim_movement_latent=512, dim_word=300, dim_pos_ohot=15, dim_motion_hidden=1024, dim_coemb_hidden=512, joints_num=22, dim_pose=263, max_motion_length=196, radius=4, fps=20, device=device(type='cuda'), is_train=True, save_root='./checkpoints/t2m/t2m', model_dir='./checkpoints/t2m/t2m/model', meta_dir='./checkpoints/t2m/t2m/meta')
Creating UNet with text encoder: 't5'
Loading T5 model from: ./T5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 30.81it/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
wandb: Currently logged in as: ccs_covenant (ccs_covenant-hong-kong-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /data/kuimou/openANT/wandb/run-20250722_075042-x1kk5krl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run t2m_20250722-07_30d77b
wandb: ⭐️ View project at https://wandb.ai/ccs_covenant-hong-kong-university-of-science-and-technology/t2m
wandb: 🚀 View run at https://wandb.ai/ccs_covenant-hong-kong-university-of-science-and-technology/t2m/runs/x1kk5krl
Building STA connector.
[CondUnet1D] Dims: [263, 1024, 1024, 1024, 1024], Multipliers: [2, 2, 2, 2]
T2M-Unet model created successfully.
Finish building Model.

Diffusion_config:
 FrozenDict([('num_train_timesteps', 1000), ('beta_start', 0.0001), ('beta_end', 0.02), ('beta_schedule', 'linear'), ('trained_betas', None), ('variance_type', 'fixed_small'), ('clip_sample', False), ('prediction_type', 'sample'), ('thresholding', False), ('dynamic_thresholding_ratio', 0.995), ('clip_sample_range', 1.0), ('sample_max_value', 1.0), ('timestep_spacing', 'leading'), ('steps_offset', 0), ('rescale_betas_zero_snr', False), ('_use_default_values', ['steps_offset', 'timestep_spacing', 'beta_start', 'clip_sample_range', 'beta_end', 'rescale_betas_zero_snr', 'thresholding', 'dynamic_thresholding_ratio', 'trained_betas', 'sample_max_value'])])
Start experiment: 2025-07-22_07:50:43
Setting up evaluation components...

Loading Evaluation Model Wrapper (Epoch 28) Completed!!

 Loading gt_eval mode HumanML3D dataset ...
./data/t2m_std.npy
  0%|          | 0/1460 [00:00<?, ?it/s] 40%|████      | 590/1460 [00:00<00:00, 5890.33it/s] 82%|████████▏ | 1192/1460 [00:00<00:00, 5962.05it/s]100%|██████████| 1460/1460 [00:00<00:00, 5746.24it/s]
Completing loading t2m dataset
Calculating dataset hash:   0%|          | 0/1530 [00:00<?, ?it/s]Calculating dataset hash:  37%|███▋      | 570/1530 [00:00<00:00, 5698.99it/s]Calculating dataset hash:  75%|███████▍  | 1140/1530 [00:00<00:00, 5230.33it/s]Calculating dataset hash: 100%|██████████| 1530/1530 [00:00<00:00, 5173.92it/s]
[Cache]  读取已有缓存: ./text_cache/t2m/val/t5/raw_embeds.pt

 Loading eval mode HumanML3D dataset ...
./checkpoints/t2m/t2m/meta/std.npy
  0%|          | 0/1460 [00:00<?, ?it/s] 28%|██▊       | 413/1460 [00:00<00:00, 4129.76it/s] 58%|█████▊    | 840/1460 [00:00<00:00, 4210.88it/s] 87%|████████▋ | 1269/1460 [00:00<00:00, 4243.47it/s]100%|██████████| 1460/1460 [00:00<00:00, 4230.23it/s]
Completing loading t2m dataset
Evaluation components are ready.
Need to train for 523 epochs....
Traceback (most recent call last):
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/kuimou/openANT/scripts/train.py", line 73, in <module>
    trainer.train(train_datasetloader)
  File "/data/kuimou/openANT/trainers/ddpm_trainer.py", line 290, in train
    self.forward(batch_data)
  File "/data/kuimou/openANT/trainers/ddpm_trainer.py", line 102, in forward
    self.prediction = self.model(
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/kuimou/openANT/models/t2m_unet.py", line 151, in forward
    enc_text = self.sta_model(enc_text, timesteps)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/kuimou/openANT/models/submodules/sta.py", line 168, in forward
    encoder_hidden_states = self.connector(
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/kuimou/openANT/models/submodules/sta.py", line 123, in forward
    latents = p_block(x, latents, timestep_embedding=timestep_embedding)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/kuimou/openANT/models/submodules/sta.py", line 69, in forward
    kv = torch.cat([k, self.ln_2(x, timestep_embedding)], dim=1)
RuntimeError: Tensors must have same number of dimensions: got 3 and 4
Traceback (most recent call last):
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/kuimou/openANT/scripts/train.py", line 73, in <module>
    trainer.train(train_datasetloader)
  File "/data/kuimou/openANT/trainers/ddpm_trainer.py", line 290, in train
    self.forward(batch_data)
  File "/data/kuimou/openANT/trainers/ddpm_trainer.py", line 102, in forward
    self.prediction = self.model(
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/kuimou/openANT/models/t2m_unet.py", line 151, in forward
    enc_text = self.sta_model(enc_text, timesteps)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/kuimou/openANT/models/submodules/sta.py", line 168, in forward
    encoder_hidden_states = self.connector(
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/kuimou/openANT/models/submodules/sta.py", line 123, in forward
    latents = p_block(x, latents, timestep_embedding=timestep_embedding)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/kuimou/openANT/models/submodules/sta.py", line 69, in forward
    kv = torch.cat([k, self.ln_2(x, timestep_embedding)], dim=1)
RuntimeError: Tensors must have same number of dimensions: got 3 and 4
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mt2m_20250722-07_30d77b[0m at: [34mhttps://wandb.ai/ccs_covenant-hong-kong-university-of-science-and-technology/t2m/runs/x1kk5krl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250722_075042-x1kk5krl/logs[0m
[DEBUG] Motions Shape: torch.Size([64, 196, 263]), M_lens: tensor([136,  70, 196, 196, 125,  46, 196, 192, 100, 127, 196, 149,  78,  69,
        128, 196, 153,  82,  98, 196, 107, 122, 196, 110,  77, 196, 120, 153,
         97, 196,  56, 107, 100,  78, 131, 196,  80, 136, 196,  98, 196, 191,
         96, 123, 196, 196, 196,  71, 145, 196,  53, 196, 196, 196, 196, 114,
         91,  72,  59, 168, 196, 184, 196,  69], device='cuda:0'), Raw Embeds Shape: torch.Size([64, 1, 77, 2048])
Traceback (most recent call last):
  File "/data/kuimou/miniconda3/envs/openANT_test/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1199, in launch_command
    simple_launcher(args)
  File "/data/kuimou/miniconda3/envs/openANT_test/lib/python3.10/site-packages/accelerate/commands/launch.py", line 785, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/data/kuimou/miniconda3/envs/openANT_test/bin/python3.10', '-m', 'scripts.train', '--abstractor', '--use_text_cache']' returned non-zero exit status 1.
